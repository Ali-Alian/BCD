{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "288dede9-9652-496d-929c-355b01670f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import albumentations as A\n",
    "from glob import glob # Used to easily find file paths\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7b41de-890d-4fb3-80cc-702c89bcbc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REFNUM</th>\n",
       "      <th>BG</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>SEVERITY</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>RADIUS</th>\n",
       "      <th>CANCER</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REFNUM</td>\n",
       "      <td>BG</td>\n",
       "      <td>CLASS</td>\n",
       "      <td>SEVERITY</td>\n",
       "      <td>X</td>\n",
       "      <td>Y</td>\n",
       "      <td>RADIUS</td>\n",
       "      <td>1</td>\n",
       "      <td>all-mias/REFNUM.pgm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mdb001</td>\n",
       "      <td>G</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>535</td>\n",
       "      <td>425</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "      <td>all-mias/mdb001.pgm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdb002</td>\n",
       "      <td>G</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>522</td>\n",
       "      <td>280</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>all-mias/mdb002.pgm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mdb003</td>\n",
       "      <td>D</td>\n",
       "      <td>NORM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>all-mias/mdb003.pgm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdb004</td>\n",
       "      <td>D</td>\n",
       "      <td>NORM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>all-mias/mdb004.pgm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   REFNUM  BG  CLASS  SEVERITY    X    Y  RADIUS  CANCER             filepath\n",
       "0  REFNUM  BG  CLASS  SEVERITY    X    Y  RADIUS       1  all-mias/REFNUM.pgm\n",
       "1  mdb001   G   CIRC         B  535  425     197       1  all-mias/mdb001.pgm\n",
       "2  mdb002   G   CIRC         B  522  280      69       1  all-mias/mdb002.pgm\n",
       "3  mdb003   D   NORM       NaN  NaN  NaN     NaN       0  all-mias/mdb003.pgm\n",
       "4  mdb004   D   NORM       NaN  NaN  NaN     NaN       0  all-mias/mdb004.pgm"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the vsualization of the table for the dataset adding the cancer clumm and the image ref id \n",
    "df = pd.read_csv(\n",
    "    'data2.txt',\n",
    "    sep='\\s+',\n",
    "    names=['REFNUM','BG','CLASS','SEVERITY','X','Y','RADIUS'],\n",
    "    na_values=['']\n",
    ")\n",
    "# Add the binary cancer label (1 if CLASS≠NORM, else 0)\n",
    "df['CANCER'] = (df['CLASS'] != 'NORM').astype(int)\n",
    "df['filepath'] = df['REFNUM'].apply(\n",
    "    lambda id: f\"all-mias/{id}.pgm\"\n",
    ")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf538fa7-1a64-4a5a-8ed0-bb2f746da25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full size mofel base on the ROI working on the full size image \n",
    "def build_fullsize_samples(img_dir, info_df):\n",
    "    \"\"\"\n",
    "    Loads each .pgm as a full-size (e.g. 1024×1024) crop:\n",
    "      - If ROI exists: crop exactly the ROI square (2*radius)\n",
    "      - Else: use the entire image\n",
    "    Then convert to 3-channel BGR and pair with label.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for filename in os.listdir(img_dir):\n",
    "        if not filename.lower().endswith('.pgm'):\n",
    "            continue\n",
    "\n",
    "        # Lookup metadata\n",
    "        refnum = os.path.splitext(filename)[0]\n",
    "        row    = info_df[info_df['REFNUM'] == refnum]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        label = int(row['CANCER'].iloc[0])\n",
    "\n",
    "        # Read gray image\n",
    "        img = cv2.imread(os.path.join(img_dir, filename), cv2.IMREAD_GRAYSCALE)\n",
    "        h, w = img.shape\n",
    "\n",
    "        x, y, r = row['X'].iloc[0], row['Y'].iloc[0], row['RADIUS'].iloc[0]\n",
    "        if pd.notna(x) and pd.notna(y) and pd.notna(r):\n",
    "            # ROI crop\n",
    "            cx, cy, radius = int(x), h - int(y), int(r)\n",
    "            x0, x1 = max(cx-radius,0), min(cx+radius,w)\n",
    "            y0, y1 = max(cy-radius,0), min(cy+radius,h)\n",
    "            crop = img[y0:y1, x0:x1]\n",
    "        else:\n",
    "            # Full image\n",
    "            crop = img\n",
    "\n",
    "        # *** NO RESIZE STEP HERE ***\n",
    "\n",
    "        # Convert to 3-channel BGR\n",
    "        img_input = cv2.cvtColor(crop, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        samples.append((img_input, label, refnum))\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab3807-313b-41f3-840c-0f7862ac961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bd417cc-890d-4bf6-bd46-b70ee54f8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduse in size so the model are more effecient and faster to run \n",
    "def build_samples(img_dir, info_df,\n",
    "                  output_size=(224, 224),\n",
    "                  fallback_size=512):\n",
    "    \"\"\"\n",
    "    For each .pgm in img_dir, look up X/Y/RADIUS in info_df.\n",
    "    - If X/Y/RADIUS are valid numbers: crop the square ROI around (X,Y) with side=2*RADIUS.\n",
    "    - If any are NaN: crop a centered square fallback of side=fallback_size.\n",
    "    Then resize the crop to output_size, convert to RGB, and pair with cancer label.\n",
    "    Returns: list of (image_array, label) tuples.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # Loop through every file in the directory\n",
    "    for filename in os.listdir(img_dir):\n",
    "        if not filename.lower().endswith('.pgm'):\n",
    "            continue\n",
    "\n",
    "        # 1) Load metadata row for this image\n",
    "        refnum = os.path.splitext(filename)[0]      # e.g. 'mdb001'\n",
    "        row   = info_df[info_df['REFNUM'] == refnum]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        label = int(row['CANCER'].iloc[0])         # 0 or 1\n",
    "\n",
    "        # 2) Read the grayscale image\n",
    "        path = os.path.join(img_dir, filename)\n",
    "        img  = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        h, w = img.shape                           # should be 1024×1024\n",
    "\n",
    "        # 3) Extract ROI if available\n",
    "        x, y, r = row['X'].iloc[0], row['Y'].iloc[0], row['RADIUS'].iloc[0]\n",
    "\n",
    "        if pd.notna(x) and pd.notna(y) and pd.notna(r):\n",
    "            # --- VALID ROI PATH ---\n",
    "            # Convert (x, y) from bottom-left origin to NumPy row/col:\n",
    "            cx     = int(x)\n",
    "            cy     = h - int(y)\n",
    "            radius = int(r)\n",
    "\n",
    "            # Define square bounds around the circle\n",
    "            x0 = max(cx - radius, 0)\n",
    "            x1 = min(cx + radius, w)\n",
    "            y0 = max(cy - radius, 0)\n",
    "            y1 = min(cy + radius, h)\n",
    "\n",
    "            crop = img[y0:y1, x0:x1]\n",
    "\n",
    "        else:\n",
    "            # --- MISSING ROI PATH ---\n",
    "            # Center of image\n",
    "            cx, cy = w // 2, h // 2\n",
    "            half   = fallback_size // 2\n",
    "\n",
    "            x0 = max(cx - half, 0)\n",
    "            x1 = min(cx + half, w)\n",
    "            y0 = max(cy - half, 0)\n",
    "            y1 = min(cy + half, h)\n",
    "\n",
    "            crop = img[y0:y1, x0:x1]\n",
    "\n",
    "        # 4) Resize everything to CNN input size\n",
    "        resized = cv2.resize(crop, output_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # 5) Convert to 3-channel (if using a pre-trained RGB model)\n",
    "        img_input = cv2.cvtColor(resized, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # 6) Store image + label\n",
    "        samples.append((img_input, label, refnum))\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1eeb13-3b5e-4129-963e-7e334b1fdb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_samples_uniform(img_dir, info_df,\n",
    "                          output_size=(1024, 1024)):  # choose full size here\n",
    "    samples = []\n",
    "    for filename in os.listdir(img_dir):\n",
    "        if not filename.lower().endswith('.pgm'):\n",
    "            continue\n",
    "\n",
    "        refnum = os.path.splitext(filename)[0]\n",
    "        row    = info_df[info_df['REFNUM'] == refnum]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        label = int(row['CANCER'].iloc[0])\n",
    "\n",
    "        # load full grayscale\n",
    "        img = cv2.imread(os.path.join(img_dir, filename), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # **Ignore ROI entirely**; use whole image\n",
    "        crop = img\n",
    "\n",
    "        # **Always** resize\n",
    "        resized = cv2.resize(crop, output_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # to 3-ch\n",
    "        img_input = cv2.cvtColor(resized, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        samples.append((img_input, label, refnum))\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb84a88-ef13-4ebc-8ea0-c336c668961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "def refnum_p_id(refnum):\n",
    "    num = int(refnum.replace('mdb', ''))\n",
    "    return math.ceil(num / 2) - 1\n",
    "    \n",
    "img_dir = 'all-mias'\n",
    "info_df = df\n",
    "\n",
    "# # Use it:\n",
    "samples = build_samples_uniform(img_dir, info_df)\n",
    "\n",
    "# Now unpack:\n",
    "images, labels, refnums = zip(*samples)\n",
    "\n",
    "# Compute group for each refnum:\n",
    "groups = [refnum_p_id(r) for r in refnums]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3fb090-597a-4144-aa5f-a41165804ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(splitter.split(images, labels, groups=groups))\n",
    "\n",
    "# Build your final train/val lists\n",
    "X_train = [images[i] for i in train_idx]\n",
    "y_train = [labels[i] for i in train_idx]\n",
    "X_val   = [images[i] for i in val_idx]\n",
    "y_val   = [labels[i] for i in val_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4301c49f-e77a-4b10-9a7e-8872df60afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MammogramSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, images, labels, batch_size=16, augment=False):\n",
    "        \"\"\"\n",
    "        images: list of NumPy arrays, one per sample\n",
    "        labels: list/array of 0/1 labels\n",
    "        batch_size: how many samples per batch\n",
    "        augment: whether to apply random transforms\n",
    "        \"\"\"\n",
    "        self.images   = images\n",
    "        self.labels   = np.array(labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.augment    = augment\n",
    "        self.indices = np.arange(len(images))\n",
    "\n",
    "    def __len__(self):\n",
    "        # how many batches in one epoch?\n",
    "        return int(np.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Called by Keras during training/validation.\n",
    "        idx: batch index [0 .. __len__()-1]\n",
    "        \"\"\"\n",
    "        # 1) select the slice of indices for this batch\n",
    "        batch_idxs = self.indices[idx*self.batch_size : (idx+1)*self.batch_size]\n",
    "\n",
    "        batch_x = []\n",
    "        for i in batch_idxs:\n",
    "            img = self.images[i].astype('float32') / 255.0  # normalize\n",
    "            if self.augment:\n",
    "                # Apply random flips\n",
    "                img = tf.image.random_flip_left_right(img)\n",
    "                img = tf.image.random_flip_up_down(img)\n",
    "                # Random brightness/contrast\n",
    "                img = tf.image.random_brightness(img, 0.1)\n",
    "                img = tf.image.random_contrast(img, 0.1, 0.2)\n",
    "            batch_x.append(img)\n",
    "\n",
    "        # 2) stack into one array of shape (batch_size, H, W, C)\n",
    "        batch_x = np.stack(batch_x, axis=0)\n",
    "\n",
    "        # 3) select corresponding labels\n",
    "        batch_y = self.labels[batch_idxs]\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle your data at the end of each epoch (if training)\n",
    "        if self.augment:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "train_seq = MammogramSequence(X_train, y_train, batch_size=16, augment=True)\n",
    "val_seq   = MammogramSequence(X_val,   y_val,   batch_size=16, augment=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c81b707-a725-40d6-96e7-257a1b500f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# 1) Load the base model (without its top), freeze it initially\n",
    "base = ResNet50(include_top=False,\n",
    "                weights='imagenet',\n",
    "                input_shape=(1024,1024,3))\n",
    "base.trainable = False\n",
    "\n",
    "# 2) Add a custom head\n",
    "x = layers.GlobalAveragePooling2D()(base.output)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = models.Model(inputs=base.input, outputs=output)\n",
    "\n",
    "# 3) Compile with appropriate loss & metrics\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97493ffc-f7bf-4eb3-9712-b3ddc5bee3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 4/17\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:02\u001b[0m 19s/step - accuracy: 0.6992 - loss: 1.5445 - precision: 0.0714 - recall: 0.0278        "
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.1, patience=3\n",
    "    )\n",
    "]\n",
    "\n",
    "# Stage 1: only the head\n",
    "history1 = model.fit(\n",
    "    train_seq,\n",
    "    validation_data=val_seq,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Stage 2: unfreeze some of the base for fine-tuning\n",
    "base.trainable = True\n",
    "# Optionally freeze most layers, only fine-tune last block(s)\n",
    "for layer in base.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy','precision','recall']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_seq,\n",
    "    validation_data=val_seq,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a5026-c020-453a-9402-824dafcb656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Predictions on validation set\n",
    "y_prob = model.predict(val_seq)\n",
    "y_pred = (y_prob > 0.5).astype(int).flatten()\n",
    "y_true = np.array(y_val)\n",
    "\n",
    "# 2) Classification report\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "print(classification_report(y_true, y_pred,\n",
    "    target_names=['No Cancer','Cancer']))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_true, y_prob))\n",
    "\n",
    "# 3) Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e4222-a477-432d-90a7-62c5d53a840a",
   "metadata": {},
   "source": [
    "## The Diffrence between 244 img size and full size which is 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41688f14-aa0f-433a-a378-47c158b1cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Cancer       0.64      1.00      0.78        42\n",
      "      Cancer       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.64        66\n",
      "   macro avg       0.32      0.50      0.39        66\n",
      "weighted avg       0.40      0.64      0.49        66\n",
      "\n",
      "ROC AUC: 0.7341269841269842\n",
      "Confusion Matrix:\n",
      " [[42  0]\n",
      " [24  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 1) Predictions on validation set\n",
    "y_prob = model.predict(val_seq)\n",
    "y_pred = (y_prob > 0.5).astype(int).flatten()\n",
    "y_true = np.array(y_val)\n",
    "\n",
    "# 2) Classification report\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "print(classification_report(y_true, y_pred,\n",
    "    target_names=['No Cancer','Cancer']))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_true, y_prob))\n",
    "\n",
    "# 3) Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05dc548-b513-46ef-9532-faaf9b85d51a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
