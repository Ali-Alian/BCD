{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29de2276-7ca5-4f67-aba5-7196a654ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from glob import glob # Used to easily find file paths\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f797ca86-1ce8-4165-95fd-3393bbb80ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REFNUM</th>\n",
       "      <th>BG</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>SEVERITY</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>RADIUS</th>\n",
       "      <th>CANCER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REFNUM</td>\n",
       "      <td>BG</td>\n",
       "      <td>CLASS</td>\n",
       "      <td>SEVERITY</td>\n",
       "      <td>X</td>\n",
       "      <td>Y</td>\n",
       "      <td>RADIUS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mdb001</td>\n",
       "      <td>G</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>535</td>\n",
       "      <td>425</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdb002</td>\n",
       "      <td>G</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>522</td>\n",
       "      <td>280</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mdb003</td>\n",
       "      <td>D</td>\n",
       "      <td>NORM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdb004</td>\n",
       "      <td>D</td>\n",
       "      <td>NORM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   REFNUM  BG  CLASS  SEVERITY    X    Y  RADIUS  CANCER\n",
       "0  REFNUM  BG  CLASS  SEVERITY    X    Y  RADIUS       0\n",
       "1  mdb001   G   CIRC         B  535  425     197       1\n",
       "2  mdb002   G   CIRC         B  522  280      69       1\n",
       "3  mdb003   D   NORM       NaN  NaN  NaN     NaN       0\n",
       "4  mdb004   D   NORM       NaN  NaN  NaN     NaN       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['REFNUM', 'BG', 'CLASS', 'SEVERITY', 'X', 'Y', 'RADIUS']\n",
    "df = pd.read_csv('data2.txt', sep=\"\\s+\", names=col_names, header=None)\n",
    "df['CANCER'] = df['SEVERITY'].apply(lambda x: 1 if x in ['B', 'M'] else 0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad777b0-eef8-4c64-b46f-fb26c3598548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a7d7fa7-35fa-4e81-9b3c-40b0b078c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_labeling(img_files, txt_path):\n",
    "    full_data = []\n",
    "    cordinates = []\n",
    "    # count_cancer = 0\n",
    "    # img_coordinate = defaultdict(list)\n",
    "    \n",
    "    for i, filename in enumerate(sorted(os.listdir(img_files))): # Opens the image file and go throuth all the image \n",
    "        if filename.endswith(\".pgm\"): # display only if the image is pgm\n",
    "            image_path = os.path.join(img_files, filename) # Getting the Image path EX => all-mias\\mdb001.pgm\n",
    "            text = txt_file[i].strip() # spliting the data in the text file \n",
    "            pairing = {\"Image\": image_path, \"Text\": text} # putting both image and the text in the dictinory \n",
    "            full_data.append(pairing) # adding all the data to the list \n",
    "\n",
    "    for pairing in full_data:\n",
    "        txt_value = pairing['Text'] # ceperating the image with the text\n",
    "        img_value = pairing['Image']\n",
    "        \n",
    "        img = cv2.imread(img_value) # creating the array\n",
    "        txt_parts = txt_value.split() # spleting text in to multiple in array so to filter the data which they have the cordinates\n",
    "        \n",
    "        # Converting the image to GRAY and then to RGB and prepare for drawing\n",
    "        img_gray = cv2.imread(img_value, cv2.IMREAD_GRAYSCALE)  \n",
    "        img_rgb = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)\n",
    "       \n",
    "        \n",
    "        # Cheking if the text line contain the cordinate or not\n",
    "        if len(txt_parts) == 7 and img is not None: \n",
    "            # Getting the cordinate for each image one by one \n",
    "\n",
    "            get_txt_data = txt_parts[4] + \" \" + txt_parts[5] + \" \" +txt_parts[6] # joining the X , Y , R\n",
    "            x, y, r = map(int, get_txt_data.split())\n",
    "            \n",
    "            y_adj = 1024 - y\n",
    "            cv2.circle(img_rgb, (x, y_adj), r, (0,255,0), 3)\n",
    "            \n",
    "            mask = np.zeros(img_rgb.shape[:2], dtype=np.uint8)\n",
    "            cv2.circle(mask, (x, y_adj), r, 255, -1)\n",
    "            roi = cv2.bitwise_and(img_rgb, img_rgb, mask=mask)\n",
    "\n",
    "            # plt.imshow(img_rgb, cmap='gray')\n",
    "            # plt.title(\"Example .pgm Image\")\n",
    "            # plt.axis('off')  # Hide axis ticks\n",
    "            # plt.show()\n",
    "\n",
    "# reading the Image file \n",
    "images_path = \"all-mias\"\n",
    "\n",
    "# reading the txt file \n",
    "txt_path = \"data2.txt\"\n",
    "with open(txt_path, \"r\") as file:\n",
    "    txt_file = file.readlines()[1:]\n",
    "\n",
    "data_labeling(images_path, txt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2721e400-749e-4467-a227-4a8fbb40ee0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a80dd459-d460-4aa5-bb82-81488842eb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training with Albumentations and tf.data pipeline...\n",
      "Epoch 1/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.6711 - auc: 0.4292 - loss: 0.6774 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6623\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 953ms/step - accuracy: 0.6445 - auc: 0.4574 - loss: 0.6612 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6555\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 958ms/step - accuracy: 0.6404 - auc: 0.4765 - loss: 0.6558 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6531\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 965ms/step - accuracy: 0.6684 - auc: 0.4282 - loss: 0.6438 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6529\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 948ms/step - accuracy: 0.6520 - auc: 0.4633 - loss: 0.6488 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6530\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 956ms/step - accuracy: 0.6489 - auc: 0.5106 - loss: 0.6481 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6530\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 953ms/step - accuracy: 0.6641 - auc: 0.5297 - loss: 0.6367 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6530\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 966ms/step - accuracy: 0.6267 - auc: 0.4949 - loss: 0.6619 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6529\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 953ms/step - accuracy: 0.6508 - auc: 0.4590 - loss: 0.6507 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6529\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 954ms/step - accuracy: 0.6437 - auc: 0.4914 - loss: 0.6528 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6529\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 945ms/step - accuracy: 0.6885 - auc: 0.5071 - loss: 0.6261 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6529\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 950ms/step - accuracy: 0.6558 - auc: 0.5162 - loss: 0.6451 - val_accuracy: 0.6406 - val_auc: 0.5870 - val_loss: 0.6529\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 956ms/step - accuracy: 0.6594 - auc: 0.4614 - loss: 0.6434 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6529\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 955ms/step - accuracy: 0.6618 - auc: 0.4636 - loss: 0.6448 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6529\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 960ms/step - accuracy: 0.6413 - auc: 0.4883 - loss: 0.6537 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6529\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 961ms/step - accuracy: 0.6255 - auc: 0.4980 - loss: 0.6642 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6529\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 943ms/step - accuracy: 0.6425 - auc: 0.5087 - loss: 0.6500 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6529\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 952ms/step - accuracy: 0.6792 - auc: 0.4230 - loss: 0.6364 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6530\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 956ms/step - accuracy: 0.6743 - auc: 0.5488 - loss: 0.6293 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6531\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 949ms/step - accuracy: 0.6911 - auc: 0.4788 - loss: 0.6236 - val_accuracy: 0.6406 - val_auc: 0.5000 - val_loss: 0.6531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_DIR = 'dataset_for_training/train/'\n",
    "VALIDATION_DIR = 'dataset_for_training/validation/'\n",
    "\n",
    "# --- Step 1: Define the Albumentations Transform Pipeline ---\n",
    "# This is where you define all your desired augmentations.\n",
    "# These will be applied to the training images only.\n",
    "# This is a powerful set of augmentations suitable for medical images.\n",
    "train_transform = A.Compose([\n",
    "    # Geometric transformations\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.7),\n",
    "    A.ElasticTransform(p=0.3, alpha=50, sigma=5),\n",
    "    A.Normalize(mean=[0.5], std=[0.5]),\n",
    "    A.GridDistortion(p=0.5),\n",
    "    \n",
    "    # Brightness and contrast transformations\n",
    "    A.RandomBrightnessContrast(\n",
    "    brightness_limit=(-0.2, 0.3),  # Range\n",
    "    contrast_limit=(-0.1, 0.1),    # Range\n",
    "    p=1.0\n",
    "    ),\n",
    "    A.CLAHE(p=0.8), # This is excellent for enhancing contrast in medical images\n",
    "    \n",
    "    # Noise and blur\n",
    "    A.GaussNoise(p=0.5),\n",
    "    A.Blur(blur_limit=3, p=0.5),\n",
    "    # A.ToTensorV2()\n",
    "])\n",
    "\n",
    "# For validation, we only need to resize and rescale. No random augmentation.\n",
    "validation_transform = A.Compose([\n",
    "    # Validation data should not be augmented randomly\n",
    "])\n",
    "\n",
    "\n",
    "# --- Step 2: Build the tf.data Pipeline (Replaces ImageDataGenerator) ---\n",
    "\n",
    "# First, get all the file paths and their corresponding labels.\n",
    "train_image_paths = glob(os.path.join(TRAIN_DIR, '*/*.png'))\n",
    "validation_image_paths = glob(os.path.join(VALIDATION_DIR, '*/*.png'))\n",
    "\n",
    "# Create labels from the folder names (0 for 'cancer', 1 for 'normal')\n",
    "# Note: Keras's flow_from_directory sorts class names alphabetically.\n",
    "# 'cancer' comes before 'normal', so Keras assigns it class 0.\n",
    "train_labels = [0 if 'cancer' in path else 1 for path in train_image_paths]\n",
    "validation_labels = [0 if 'cancer' in path else 1 for path in validation_image_paths]\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path, label):\n",
    "    \"\"\"Loads, decodes, and resizes an image.\"\"\"\n",
    "    # Read the image file\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode to a tensor. We specify 3 channels as the model expects it.\n",
    "    image = tf.io.decode_png(image, channels=3)\n",
    "    # Resize the image to the target size\n",
    "    image = tf.image.resize(image, [IMG_SIZE[0], IMG_SIZE[1]])\n",
    "    # Rescale pixel values to [0, 1]\n",
    "    image = image / 255.0\n",
    "    return image, label\n",
    "\n",
    "def apply_augmentations(image, label, transform):\n",
    "    \"\"\"A wrapper function to apply Albumentations transforms within TensorFlow.\"\"\"\n",
    "    def augment(img):\n",
    "        aug_data = transform(image=img.numpy())\n",
    "        return aug_data['image']\n",
    "\n",
    "    # Use tf.py_function to run the python-based Albumentations library\n",
    "    # The [image] and Tout=[tf.float32] define the input and output types.\n",
    "    aug_image = tf.py_function(func=augment, inp=[image], Tout=tf.float32)\n",
    "    # Make sure the output shape is set correctly\n",
    "    aug_image.set_shape([IMG_SIZE[0], IMG_SIZE[1], 3])\n",
    "    return aug_image, label\n",
    "\n",
    "\n",
    "# Create the final training dataset object\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_labels))\n",
    "train_dataset = (\n",
    "    train_dataset.shuffle(buffer_size=len(train_image_paths)) # Shuffle the data\n",
    "    .map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE) # Load and resize\n",
    "    .map(lambda x, y: apply_augmentations(x, y, train_transform), num_parallel_calls=tf.data.AUTOTUNE) # Apply augmentations\n",
    "    .batch(BATCH_SIZE) # Create batches\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE) # Pre-load the next batch for performance\n",
    ")\n",
    "\n",
    "# Create the final validation dataset object (without augmentation)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_image_paths, validation_labels))\n",
    "validation_dataset = (\n",
    "    validation_dataset\n",
    "    .map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE) # Just load and resize\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# --- The rest of your code is now UNCHANGED ---\n",
    "\n",
    "# --- 3. Build the Model with Transfer Learning (Unchanged) ---\n",
    "base_model = tf.keras.applications.EfficientNetB0(weights='imagenet', include_top=False, input_shape=(*IMG_SIZE, 3))\n",
    "base_model.trainable = False\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "predictions = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# --- 4. Compile the Model (Unchanged) ---\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "# --- 5. Train the Model (Updated to use the new datasets) ---\n",
    "print(\"Starting model training with Albumentations and tf.data pipeline...\")\n",
    "history = model.fit(\n",
    "    train_dataset, # <-- Use the new training dataset\n",
    "    epochs=20,\n",
    "    validation_data=validation_dataset # <-- Use the new validation dataset\n",
    ")\n",
    "\n",
    "# --- 6. Save your trained model (Unchanged) ---\n",
    "model.save('breast_cancer_classifier_v2_albumentations.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7dd5185-29b6-45b7-a19f-c6d52e4313be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = [].\n",
    "# for file in img_files:\n",
    "#     image = Image.open(file)\n",
    "#     img.append(image)\n",
    "# # images = [Image.open(file) for file in img_files]\n",
    "\n",
    "# columns = 5\n",
    "# rows = len(img) // columns + int(len(img) % columns !=0)\n",
    "\n",
    "# fig, axes = plt.subplots(rows, columns, figsize=(15,15))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for idx, image in enumerate(img):\n",
    "#     axes[idx].imshow(image, cmap='gray')\n",
    "#     axes[idx].axis('off')\n",
    "#     axes[idx].set_title(f\"Image {idx+1}\")\n",
    "\n",
    "# for ax in axes[len(img):]:\n",
    "#     ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"found {len(img_files)} .pgm images\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "#####################################################################\n",
    "# cols = 5\n",
    "# rows = len(images) // cols + int(len(images) % cols != 0)\n",
    "\n",
    "# # Create subplots\n",
    "# # fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "# fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "# axes = axes.flatten()  # Flatten in case of 2D array of axes\n",
    "\n",
    "# # Loop through images and plot each one\n",
    "# for idx, img in enumerate(images):\n",
    "#     axes[idx].imshow(img, cmap='gray')\n",
    "#     axes[idx].axis('off')\n",
    "#     axes[idx].set_title(f\"Image {idx+1}\")\n",
    "\n",
    "# # Turn off any unused subplots\n",
    "# for ax in axes[len(images):]:\n",
    "#     ax.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b047e-a933-464b-a99d-fcd1e88f7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reading the Image file \n",
    "# img_files = glob.glob(\"all-mias\")\n",
    "# # img_files = \"all-mias\"\n",
    "# print(f\"found {len(img_files)} .pmg images\")\n",
    "\n",
    "# # reading the txt file \n",
    "# txt_path = \"info.txt\"\n",
    "# with open(txt_path, \"r\") as file:\n",
    "#     txt_file = file.readlines()\n",
    "\n",
    "# first_img = Image.open(img_files[11])\n",
    "# plt.imshow(first_img, cmap='gray')\n",
    "# plt.title(\"Example .pgm Image\")\n",
    "# plt.axis('off')  # Hide axis ticks\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dafea9-1fa5-4bac-9e71-ac2efa8bdf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_labeling(img_files, txt_path):\n",
    "#     full_data = []\n",
    "#     count_cancer = 0\n",
    "#     img_coordinate = defaultdict(list)\n",
    "#     for i, filename in enumerate(sorted(os.listdir(img_files))):\n",
    "#         if filename.endswith(\".pgm\"):\n",
    "#             image_path = os.path.join(img_files, filename)\n",
    "#             text = txt_file[i].strip()\n",
    "\n",
    "#             pairing = {\"Image\": image_path, \"Text\": text}\n",
    "#             full_data.append(pairing)\n",
    "\n",
    "#     for pairing in full_data:\n",
    "#         txt_value = pairing['Text']\n",
    "#         img_value = pairing['Image']\n",
    "#         img = cv2.imread(img_value)\n",
    "#         txt_parts = txt_value.split()\n",
    "#         # print(f\" {txt_value} {img_value}\")\n",
    "#         # cordinate = {}\n",
    "        \n",
    "#         if len(txt_parts) == 7 and img is not None:\n",
    "#             count_cancer += 1\n",
    "#             get_txt_data = txt_parts[4] + \" - \" + txt_parts[5] + \" - \" +txt_parts[6] + \" - \" +img_value[9:]\n",
    "#             # print(\" x      y    R\")\n",
    "#             # print(get_txt_data)\n",
    "#             sorted_mdb = sorted(txt_value, key=lambda item: item[-1])\n",
    "#             for x, y, r in sorted_mdb:\n",
    "#                 y_adj = 1024 - y  # adjust y-coordinate\n",
    "#                 cv2.circle(img_value, (x, y_adj), r, (0, 255, 0), 2)\n",
    "\n",
    "#         if sorted_mdb:\n",
    "#             x, y, r = sorted_mdb[0]\n",
    "#             y_adj = 1024 - y\n",
    "#             mask = np.zeros(img_value.shape[:2], dtype=np.uint8)\n",
    "#             cv2.circle(mask, (x, y_adj), r, 255, -1)\n",
    "#             roi = cv2.bitwise_and(img_value, img_value, mask=mask)\n",
    "\n",
    "\n",
    "#         plt.imshow(img_rgb, cmap='gray')\n",
    "#         plt.title(\"Example .pgm Image\")\n",
    "#         plt.axis('off')  # Hide axis ticks\n",
    "#         plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "# # reading the Image file \n",
    "# images_path = \"all-mias\"\n",
    "\n",
    "# # reading the txt file \n",
    "# txt_path = \"Info.txt\"\n",
    "# with open(txt_path, \"r\") as file:\n",
    "#     txt_file = file.readlines()\n",
    "\n",
    "# data_labeling(images_path, txt_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
