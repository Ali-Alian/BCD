{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe6a8b-ba26-4603-be32-4c58aa3219c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "improved_lbp_glcm_pipeline.py\n",
    "\n",
    "- ROI extraction (consistent normal/abnormal)\n",
    "- per-ROI CLAHE\n",
    "- LBP pooled features (adaptive bins)\n",
    "- quantized GLCM features\n",
    "- SelectKBest feature selection inside pipeline\n",
    "- SMOTE inside CV if available, else class_weight fallback\n",
    "- nested GroupKFold (inner) GridSearchCV and held-out GroupShuffleSplit\n",
    "- prints and saves results to ./results/\n",
    "\"\"\"\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# try to import imblearn (SMOTE & pipeline). If missing fallback will be used.\n",
    "USE_SMOTE = True\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "except Exception:\n",
    "    USE_SMOTE = False\n",
    "    ImbPipeline = None\n",
    "    print(\"[WARN] imbalanced-learn not available. Script will use class_weight='balanced' instead of SMOTE.\")\n",
    "\n",
    "# ----------------- USER CONFIG -----------------\n",
    "IMAGES_DIR = \"all-mias\"\n",
    "META_PATH = \"data2.txt\"\n",
    "IMAGE_SIZE = 1024               # MIAS image size reference\n",
    "TARGET_SIZE = 128               # None to keep native crop size, or int to resize (128 recommended)\n",
    "MIN_SIDE = 32                   # pad if ROI smaller\n",
    "CLAHE_CLIP = 2.0\n",
    "CLAHE_TILE = (8, 8)\n",
    "\n",
    "# LBP params\n",
    "P = 8\n",
    "# You can enable multi-scale LBP; set RB list to include desired radii:\n",
    "LBP_RADII = [1, 3]             # set to [3] or [1,3] for multiscale\n",
    "LBP_METHOD = 'uniform'\n",
    "\n",
    "# pooling grid for LBP\n",
    "POOL_G = 3                      # 3x3 pooling (try 3,4,6). smaller reduces dims\n",
    "\n",
    "# GLCM params (quantized)\n",
    "GLCM_LEVELS = 32\n",
    "GLCM_DISTANCES = [1, 3]\n",
    "GLCM_ANGLES = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "\n",
    "# CV params\n",
    "OUTER_SPLITS = 5\n",
    "INNER_SPLITS = 4\n",
    "RANDOM_STATE = 42\n",
    "HOLDOUT_TEST_SIZE = 0.2\n",
    "\n",
    "# Grid search params\n",
    "RF_PARAM_GRID = {\n",
    "    'clf__n_estimators': [100, 300],\n",
    "    'clf__max_depth': [6, 12, None],\n",
    "    'clf__min_samples_leaf': [2, 6],\n",
    "}\n",
    "SVM_PARAM_GRID = {\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__gamma': ['scale', 0.01],\n",
    "}\n",
    "\n",
    "SELECT_K = 80  # number of features to keep with SelectKBest (set based on experiments)\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "# ---------- helper functions ----------\n",
    "def read_metadata(meta_path):\n",
    "    col_names = ['REFNUM', 'BG', 'CLASS', 'SEVERITY', 'X', 'Y', 'RADIUS']\n",
    "    df = pd.read_csv(meta_path, sep=r\"\\s+\", names=col_names, header=None)\n",
    "    df['CANCER'] = df['SEVERITY'].apply(lambda x: 1 if x in ['B', 'M'] else 0)\n",
    "    return df\n",
    "\n",
    "def ref_to_patient_id(ref):\n",
    "    try:\n",
    "        n = int(''.join(ch for ch in ref if ch.isdigit()))\n",
    "        return (n - 1) // 2\n",
    "    except:\n",
    "        return ref\n",
    "\n",
    "def clamp(a, lo, hi):\n",
    "    return max(lo, min(hi, a))\n",
    "\n",
    "def crop_square(img, cx, cy, r):\n",
    "    H, W = img.shape\n",
    "    x0 = clamp(cx - r, 0, W)\n",
    "    x1 = clamp(cx + r, 0, W)\n",
    "    y0 = clamp(cy - r, 0, H)\n",
    "    y1 = clamp(cy + r, 0, H)\n",
    "    if x1 <= x0 or y1 <= y0:\n",
    "        return img.copy()\n",
    "    return img[y0:y1, x0:x1]\n",
    "\n",
    "def pad_to_min_side(img, min_side):\n",
    "    h, w = img.shape\n",
    "    top = bottom = left = right = 0\n",
    "    if h < min_side:\n",
    "        extra = min_side - h\n",
    "        top = extra // 2\n",
    "        bottom = extra - top\n",
    "    if w < min_side:\n",
    "        extra = min_side - w\n",
    "        left = extra // 2\n",
    "        right = extra - left\n",
    "    if any([top, bottom, left, right]):\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, borderType=cv2.BORDER_REFLECT)\n",
    "    return img\n",
    "\n",
    "def apply_clahe_to_roi(roi, clip=2.0, tile=(8,8)):\n",
    "    if roi.dtype != np.uint8:\n",
    "        if roi.max() <= 1.0:\n",
    "            roi_u8 = (roi * 255).astype(np.uint8)\n",
    "        else:\n",
    "            roi_u8 = roi.astype(np.uint8)\n",
    "    else:\n",
    "        roi_u8 = roi\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=tile)\n",
    "    return clahe.apply(roi_u8)\n",
    "\n",
    "def pooled_lbp_matrix(lbp_map, G=3, n_bins=None):\n",
    "    H, W = lbp_map.shape\n",
    "    row_edges = np.linspace(0, H, G+1, dtype=int)\n",
    "    col_edges = np.linspace(0, W, G+1, dtype=int)\n",
    "    rows = []\n",
    "    # use dynamic n_bins from caller\n",
    "    for i in range(G):\n",
    "        for j in range(G):\n",
    "            r0, r1 = row_edges[i], row_edges[i+1]\n",
    "            c0, c1 = col_edges[j], col_edges[j+1]\n",
    "            region = lbp_map[r0:r1, c0:c1]\n",
    "            # ensure integer codes\n",
    "            region_int = region.ravel().astype(int)\n",
    "            if n_bins is None:\n",
    "                n_bins_reg = int(region_int.max()) + 1 if region_int.size>0 else 1\n",
    "                hist = np.bincount(region_int, minlength=n_bins_reg).astype(float)\n",
    "            else:\n",
    "                hist = np.bincount(region_int, minlength=n_bins).astype(float)\n",
    "            if hist.sum() > 0:\n",
    "                hist /= hist.sum()\n",
    "            # if n_bins is provided, hist length is fixed\n",
    "            if n_bins is not None and len(hist) < n_bins:\n",
    "                # pad\n",
    "                hist = np.pad(hist, (0, n_bins - len(hist)), mode='constant')\n",
    "            rows.append(hist)\n",
    "    M = np.vstack(rows)\n",
    "    return M\n",
    "\n",
    "def quantize_img_levels(img, levels=32):\n",
    "    a = img.astype(np.float32)\n",
    "    if a.max() > 1.1:\n",
    "        a = a / 255.0\n",
    "    q = np.floor(a * (levels - 1) + 0.5).astype(np.uint8)\n",
    "    return q\n",
    "\n",
    "def extract_glcm_features(patch, distances, angles, levels):\n",
    "    q = quantize_img_levels(patch, levels=levels)\n",
    "    glcm = graycomatrix(q, distances=distances, angles=angles, levels=levels, symmetric=True, normed=True)\n",
    "    props = ['contrast', 'dissimilarity', 'homogeneity', 'ASM', 'energy', 'correlation']\n",
    "    feats = []\n",
    "    for p in props:\n",
    "        mat = graycoprops(glcm, p)\n",
    "        feats.append(float(np.nanmean(mat)))\n",
    "        feats.append(float(np.nanvar(mat)))\n",
    "    return np.array(feats, dtype=float)  # length = 2 * len(props)\n",
    "\n",
    "# ---------- main ----------\n",
    "def main():\n",
    "    print(\"[INFO] Reading metadata...\")\n",
    "    df = read_metadata(META_PATH)\n",
    "    refs = sorted([f for f in os.listdir(IMAGES_DIR) if f.lower().endswith('.pgm')])\n",
    "    print(f\"[INFO] Found {len(refs)} image files in {IMAGES_DIR}\")\n",
    "    radii = pd.to_numeric(df['RADIUS'], errors='coerce').dropna()\n",
    "    median_radius = int(radii.median()) if radii.size > 0 else 48\n",
    "    print(\"[INFO] median radius:\", median_radius)\n",
    "\n",
    "    # ROI extraction\n",
    "    rois = []\n",
    "    rois_raw = []   # keep a raw copy (pre-CLAHE) for comparisons\n",
    "    labels = []\n",
    "    groups = []\n",
    "    ref_list = []\n",
    "\n",
    "    for fname in refs:\n",
    "        ref = os.path.splitext(fname)[0]\n",
    "        row = df[df['REFNUM'] == ref]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        row = row.iloc[0]\n",
    "        img_path = os.path.join(IMAGES_DIR, fname)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(\"[WARN] cannot read\", img_path); continue\n",
    "\n",
    "        label = int(row['CANCER'])\n",
    "        x, y, r = row['X'], row['Y'], row['RADIUS']\n",
    "\n",
    "        if label == 1 and pd.notna(x) and pd.notna(y) and pd.notna(r):\n",
    "            cx = int(float(x))\n",
    "            cy = int(IMAGE_SIZE - float(y))  # convert bottom-left MIAS to top-left indexing\n",
    "            rr = int(max(int(r), 8))\n",
    "            roi = crop_square(img, cx, cy, rr)\n",
    "        else:\n",
    "            cx = img.shape[1] // 2\n",
    "            cy = img.shape[0] // 2\n",
    "            rr = median_radius\n",
    "            roi = crop_square(img, cx, cy, rr)\n",
    "\n",
    "        roi = pad_to_min_side(roi, MIN_SIDE)\n",
    "        rois_raw.append(roi.copy())\n",
    "        roi_clahe = apply_clahe_to_roi(roi, clip=CLAHE_CLIP, tile=CLAHE_TILE)\n",
    "        if TARGET_SIZE is not None:\n",
    "            if roi_clahe.shape[0] > TARGET_SIZE or roi_clahe.shape[1] > TARGET_SIZE:\n",
    "                interp = cv2.INTER_AREA\n",
    "            else:\n",
    "                interp = cv2.INTER_LINEAR\n",
    "            roi_clahe = cv2.resize(roi_clahe, (TARGET_SIZE, TARGET_SIZE), interpolation=interp)\n",
    "\n",
    "        rois.append(roi_clahe)\n",
    "        labels.append(label)\n",
    "        groups.append(ref_to_patient_id(ref))\n",
    "        ref_list.append(ref)\n",
    "\n",
    "    print(f\"[INFO] Extracted {len(rois)} ROIs (CLAHE applied per-ROI).\")\n",
    "    if len(rois) == 0:\n",
    "        raise RuntimeError(\"No ROIs extracted - check dataset paths and metadata.\")\n",
    "\n",
    "    # Compute LBP maps for each radius and collect global max code to set fixed bins\n",
    "    print(\"[INFO] Computing LBP maps for radii:\", LBP_RADII)\n",
    "    lbp_maps_per_radius = {r: [] for r in LBP_RADII}\n",
    "    max_code = 0\n",
    "    for roi in rois:\n",
    "        # ensure float input\n",
    "        img_float = roi.astype(np.float32)\n",
    "        for r in LBP_RADII:\n",
    "            lbp_map = local_binary_pattern(img_float, P, r, method=LBP_METHOD)\n",
    "            # LBP returns floats; convert to ints safely\n",
    "            lbp_int = np.round(lbp_map).astype(int)\n",
    "            lbp_maps_per_radius[r].append(lbp_int)\n",
    "            max_code = max(max_code, int(lbp_int.max()))\n",
    "\n",
    "    n_lbp_bins = max_code + 1\n",
    "    print(f\"[INFO] Determined LBP n_bins = {n_lbp_bins}\")\n",
    "\n",
    "    # Build feature matrices\n",
    "    X_lbp_parts = []  # will contain per-radius pooled LBP flattened\n",
    "    X_glcm = []\n",
    "    print(\"[INFO] Building pooled LBP histograms and GLCM features for each ROI...\")\n",
    "    for idx, roi in enumerate(rois):\n",
    "        roi_uint8 = roi.astype(np.uint8)\n",
    "        # LBP pooled for each radius, concatenate\n",
    "        lbp_concat = []\n",
    "        for r in LBP_RADII:\n",
    "            lbp_map = lbp_maps_per_radius[r][idx]\n",
    "            M = pooled_lbp_matrix(lbp_map, G=POOL_G, n_bins=n_lbp_bins)  # shape (G*G, n_bins)\n",
    "            lbp_concat.append(M.ravel())\n",
    "        lbp_vec = np.hstack(lbp_concat)\n",
    "        X_lbp_parts.append(lbp_vec)\n",
    "\n",
    "        # GLCM features (quantized)\n",
    "        glcm_feats = extract_glcm_features(roi_uint8, distances=GLCM_DISTANCES, angles=GLCM_ANGLES, levels=GLCM_LEVELS)\n",
    "        X_glcm.append(glcm_feats)\n",
    "\n",
    "    X_lbp = np.vstack(X_lbp_parts)\n",
    "    X_glcm = np.vstack(X_glcm)\n",
    "    X = np.hstack([X_glcm, X_lbp])\n",
    "    y = np.array(labels)\n",
    "    groups_arr = np.array(groups)\n",
    "\n",
    "    print(\"[INFO] Feature shapes: X_glcm\", X_glcm.shape, \"X_lbp\", X_lbp.shape, \"combined X\", X.shape)\n",
    "    print(\"Label counts:\", Counter(y))\n",
    "    # safety check\n",
    "    if np.isnan(X).any():\n",
    "        raise RuntimeError(\"NaN found in features!\")\n",
    "\n",
    "    # Create feature names for mapping importances\n",
    "    glcm_props = ['contrast', 'dissimilarity', 'homogeneity', 'ASM', 'energy', 'correlation']\n",
    "    glcm_names = []\n",
    "    for p in glcm_props:\n",
    "        glcm_names += [f\"{p}_mean\", f\"{p}_var\"]\n",
    "    lbp_names = []\n",
    "    region_count = POOL_G * POOL_G\n",
    "    for r in LBP_RADII:\n",
    "        for region_ix in range(region_count):\n",
    "            for code in range(n_lbp_bins):\n",
    "                lbp_names.append(f\"LBP_R{r}_r{region_ix:02d}_c{code:03d}\")\n",
    "    feature_names = glcm_names + lbp_names\n",
    "    assert X.shape[1] == len(feature_names)\n",
    "\n",
    "    # ---------------- train/test split (group-aware held-out) ----------------\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=HOLDOUT_TEST_SIZE, random_state=RANDOM_STATE)\n",
    "    train_idx, test_idx = next(gss.split(X, y, groups_arr))\n",
    "    X_tr, X_te = X[train_idx], X[test_idx]\n",
    "    y_tr, y_te = y[train_idx], y[test_idx]\n",
    "    groups_tr, groups_te = groups_arr[train_idx], groups_arr[test_idx]\n",
    "    print(\"[INFO] Train samples:\", X_tr.shape[0], \"Test samples:\", X_te.shape[0])\n",
    "\n",
    "    # Build pipelines: keep feature selection inside pipeline (SelectKBest)\n",
    "    selector = SelectKBest(mutual_info_classif, k=min(SELECT_K, X_tr.shape[1]))\n",
    "\n",
    "    # If SMOTE available, put inside ImbPipeline, else use sklearn Pipeline and class_weight\n",
    "    if USE_SMOTE:\n",
    "        rf_pipeline = ImbPipeline([\n",
    "            ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('select', selector),\n",
    "            ('clf', RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced', n_jobs=-1))\n",
    "        ])\n",
    "        svm_pipeline = ImbPipeline([\n",
    "            ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('select', selector),\n",
    "            ('clf', SVC(probability=True, class_weight='balanced', random_state=RANDOM_STATE))\n",
    "        ])\n",
    "    else:\n",
    "        rf_pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('select', selector),\n",
    "            ('clf', RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced', n_jobs=-1))\n",
    "        ])\n",
    "        svm_pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('select', selector),\n",
    "            ('clf', SVC(probability=True, class_weight='balanced', random_state=RANDOM_STATE))\n",
    "        ])\n",
    "\n",
    "    # inner CV uses GroupKFold on training set\n",
    "    def run_gridsearch(pipeline, param_grid, X_train, y_train, groups_train, name=\"model\"):\n",
    "        inner_cv = GroupKFold(n_splits=INNER_SPLITS)\n",
    "        gs = GridSearchCV(pipeline, param_grid, cv=inner_cv.split(X_train, y_train, groups_train),\n",
    "                          scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "        gs.fit(X_train, y_train)\n",
    "        print(f\"[INFO] Best {name} params:\", gs.best_params_)\n",
    "        return gs.best_estimator_, gs\n",
    "\n",
    "    print(\"[INFO] Running inner grid-search for RandomForest...\")\n",
    "    best_rf, rf_gs = run_gridsearch(rf_pipeline, RF_PARAM_GRID, X_tr, y_tr, groups_tr, name=\"RandomForest\")\n",
    "\n",
    "    print(\"[INFO] Running inner grid-search for SVM...\")\n",
    "    best_svm, svm_gs = run_gridsearch(svm_pipeline, SVM_PARAM_GRID, X_tr, y_tr, groups_tr, name=\"SVM\")\n",
    "\n",
    "    # Evaluate on held-out test set\n",
    "    def evaluate_model(est, X_test, y_test, name=\"model\"):\n",
    "        y_pred = est.predict(X_test)\n",
    "        if hasattr(est, \"predict_proba\"):\n",
    "            try:\n",
    "                y_prob = est.predict_proba(X_test)[:, 1]\n",
    "            except Exception:\n",
    "                y_prob = None\n",
    "        else:\n",
    "            y_prob = None\n",
    "        print(f\"\\n=== Held-out evaluation: {name} ===\")\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"Recall:\", recall_score(y_test, y_pred, zero_division=0))\n",
    "        if y_prob is not None and len(np.unique(y_test)) > 1:\n",
    "            print(\"AUC:\", roc_auc_score(y_test, y_prob))\n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        return y_pred, y_prob\n",
    "\n",
    "    ypred_rf, yprob_rf = evaluate_model(best_rf, X_te, y_te, name=\"RandomForest (best)\")\n",
    "    ypred_svm, yprob_svm = evaluate_model(best_svm, X_te, y_te, name=\"SVM (best)\")\n",
    "\n",
    "    # Feature importances mapping (RF)\n",
    "    try:\n",
    "        rf_clf = best_rf.named_steps['clf'] if USE_SMOTE else best_rf.named_steps['clf']\n",
    "        importances = rf_clf.feature_importances_\n",
    "        # map to names\n",
    "        idxs = np.argsort(importances)[::-1][:30]\n",
    "        print(\"\\nTop 30 feature importances (index, name, importance):\")\n",
    "        for idx in idxs:\n",
    "            print(idx, feature_names[idx], f\"{importances[idx]:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Could not extract feature importances:\", e)\n",
    "\n",
    "    # Threshold tuning on held-out set to prioritize recall if desired (example)\n",
    "    if yprob_rf is not None:\n",
    "        best_thresh = 0.5\n",
    "        best_recall = recall_score(y_te, (yprob_rf >= 0.5).astype(int))\n",
    "        # choose threshold that maximizes recall while keeping precision >= 0.5 (example)\n",
    "        for t in np.linspace(0.1, 0.9, 81):\n",
    "            preds_t = (yprob_rf >= t).astype(int)\n",
    "            r = recall_score(y_te, preds_t, zero_division=0)\n",
    "            # for illustration choose threshold maximizing recall\n",
    "            if r > best_recall:\n",
    "                best_recall = r; best_thresh = t\n",
    "        print(f\"[INFO] RF held-out best threshold for recall (example): {best_thresh:.2f}, recall={best_recall:.3f}\")\n",
    "        # show confusion at best threshold\n",
    "        preds_best = (yprob_rf >= best_thresh).astype(int)\n",
    "        print(\"Confusion at chosen threshold:\\n\", confusion_matrix(y_te, preds_best))\n",
    "        print(classification_report(y_te, preds_best))\n",
    "\n",
    "    # Save outputs\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    with open(os.path.join(\"results\", \"features_X_y_groups.pkl\"), \"wb\") as f:\n",
    "        pickle.dump({\"X\": X, \"y\": y, \"groups\": groups_arr, \"ref_list\": ref_list, \"feature_names\": feature_names}, f)\n",
    "    with open(os.path.join(\"results\", \"best_rf.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(best_rf, f)\n",
    "    with open(os.path.join(\"results\", \"best_svm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(best_svm, f)\n",
    "\n",
    "    print(\"\\n[INFO] Done. Results and models saved to ./results/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
