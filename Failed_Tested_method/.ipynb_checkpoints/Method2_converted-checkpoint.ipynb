{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1189f5ec",
   "metadata": {},
   "source": [
    "# Method2 converted to Method1 structure\n",
    "\n",
    "This notebook contains Method2 logic restructured to follow the Method1 cell layout.\n",
    "\n",
    "**Notes:**\n",
    "- GLCM features are computed first, then LBP, then intensity stats.\n",
    "- Pipelines and GroupKFold usage are included as in Method1 structure.\n",
    "- Run cells sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: imports\n",
    "import os, math, time, pickle, json\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import skew\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV, GroupShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea690eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: Metadata & helpers\n",
    "def read_metadata(meta_path):\n",
    "    # robust whitespace-delimited read; uppercase column names\n",
    "    df = pd.read_csv(meta_path, delim_whitespace=True, dtype=str)\n",
    "    df.columns = [c.strip().upper() for c in df.columns]\n",
    "    # try known label columns\n",
    "    if 'SEVERITY' in df.columns:\n",
    "        df['CANCER'] = df['SEVERITY'].map({'B':1, 'M':1}).fillna(0).astype(int)\n",
    "    elif 'CLASS' in df.columns:\n",
    "        df['CANCER'] = df['CLASS'].map({'B':1, 'M':1}).fillna(0).astype(int)\n",
    "    else:\n",
    "        # fallback: if a numeric label column exists, try to use it\n",
    "        for col in ['LABEL', 'DIAGNOSIS', 'PATHOLOGY']:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    df['CANCER'] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if 'CANCER' not in df.columns:\n",
    "            raise ValueError(\"Cannot find SEVERITY/CLASS or equivalent label column in metadata.\")\n",
    "    # numeric conversions\n",
    "    for c in ['RADIUS','X','Y']:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "        else:\n",
    "            df[c] = pd.NA\n",
    "    # patient id inference from REFNUM (e.g., mdb001/mdb002 -> patient grouping)\n",
    "    def _ref_to_patient(ref):\n",
    "        try:\n",
    "            s = str(ref)\n",
    "            digits = ''.join(ch for ch in s if ch.isdigit())\n",
    "            if digits == '':\n",
    "                return None\n",
    "            n = int(digits)\n",
    "            return ((n-1)//2) + 1\n",
    "        except Exception:\n",
    "            return None\n",
    "    if 'REFNUM' in df.columns:\n",
    "        df['patient_id'] = df['REFNUM'].map(_ref_to_patient)\n",
    "    else:\n",
    "        df['patient_id'] = None\n",
    "    return df\n",
    "\n",
    "def preprocess_img(img):\n",
    "    # CLAHE equalization; expects grayscale uint8 input\n",
    "    if img is None:\n",
    "        return None\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    return clahe.apply(img)\n",
    "\n",
    "def roi_from_row(img_eq, row, image_size=1024, median_radius=48, min_side=32):\n",
    "    # Return square ROI; if lesion coords available use them, else center crop using median_radius\n",
    "    label = int(row['CANCER']) if 'CANCER' in row else 0\n",
    "    x = row.get('X', None); y = row.get('Y', None); r = row.get('RADIUS', None)\n",
    "    H, W = img_eq.shape\n",
    "    if label == 1 and pd.notna(x) and pd.notna(y) and pd.notna(r):\n",
    "        cx = int(x)\n",
    "        cy = int(image_size - float(y))  # convert MIAS bottom-left -> top-left\n",
    "        radius = int(r)\n",
    "        x0 = max(0, cx-radius); x1 = min(W, cx+radius)\n",
    "        y0 = max(0, cy-radius); y1 = min(H, cy+radius)\n",
    "        roi = img_eq[y0:y1, x0:x1]\n",
    "        if roi.size == 0:\n",
    "            roi = img_eq.copy()\n",
    "    else:\n",
    "        radius = int(median_radius)\n",
    "        cx, cy = W//2, H//2\n",
    "        x0 = max(0, cx-radius); x1 = min(W, cx+radius)\n",
    "        y0 = max(0, cy-radius); y1 = min(H, cy+radius)\n",
    "        roi = img_eq[y0:y1, x0:x1]\n",
    "    # pad if too small\n",
    "    h,w = roi.shape\n",
    "    if h < min_side or w < min_side:\n",
    "        top = max(0, (min_side-h)//2); bottom = max(0, min_side-h-top)\n",
    "        left = max(0, (min_side-w)//2); right = max(0, min_side-w-left)\n",
    "        roi = cv2.copyMakeBorder(roi, top, bottom, left, right, cv2.BORDER_REFLECT)\n",
    "    return roi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d87bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: Feature extraction functions (GLCM first, then LBP, then intensity stats)\n",
    "def compute_glcm_features(roi, distances=(1,), angles=(0, np.pi/4, np.pi/2, 3*np.pi/4), levels=32):\n",
    "    roi_q = (roi / (256.0/levels)).astype(np.uint8)\n",
    "    glcm = graycomatrix(roi_q, distances=list(distances), angles=list(angles), levels=levels, symmetric=True, normed=True)\n",
    "    props = ['contrast','dissimilarity','homogeneity','energy','correlation']\n",
    "    feats = [float(graycoprops(glcm, p).mean()) for p in props]\n",
    "    return np.array(feats, dtype=float)\n",
    "\n",
    "def compute_lbp_hist(roi, P=8, radii=(1,3), n_bins=59):\n",
    "    feats = []\n",
    "    for R in radii:\n",
    "        lbp = local_binary_pattern(roi, P, R, method='uniform')\n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins))\n",
    "        s = hist.sum()\n",
    "        if s == 0:\n",
    "            feats.extend([0.0]*n_bins)\n",
    "        else:\n",
    "            feats.extend((hist.astype(float)/s).tolist())\n",
    "    return np.array(feats, dtype=float)\n",
    "\n",
    "def intensity_stats(roi):\n",
    "    return np.array([float(roi.mean()), float(roi.std()), float(roi.min()), float(roi.max())], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aace29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: convert helper and sample plotting utility\n",
    "def convert(o):\n",
    "    if isinstance(o, (np.integer,)):\n",
    "        return int(o)\n",
    "    if isinstance(o, (np.floating,)):\n",
    "        return float(o)\n",
    "    if isinstance(o, (np.ndarray,)):\n",
    "        return o.tolist()\n",
    "    return str(o)\n",
    "\n",
    "def plot_sample_rois(sample_rois, outpath):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    for idx, (ref, lab, grp, roi_img) in enumerate(sample_rois):\n",
    "        ax = fig.add_subplot(2,3, idx+1)\n",
    "        ax.imshow(roi_img, cmap='gray'); ax.set_title(f\"{ref} L={lab} G={grp}\"); ax.axis('off')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outpath)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d394468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: Feature extraction loop (run this cell to extract features and save features.pkl)\n",
    "# Edit args below for paths and sizes if needed (these match Method1 CLI names)\n",
    "class Args:\n",
    "    images = \"all-mias\"\n",
    "    meta = \"data2.txt\"\n",
    "    outdir = \"results_method2\"\n",
    "    target_size = 128\n",
    "    min_side = 32\n",
    "    image_size = 1024\n",
    "    P = 8\n",
    "    lbp_radii = [1,3]\n",
    "    lbp_bins = 59\n",
    "    glcm_distances = [1]\n",
    "    glcm_levels = 32\n",
    "    select_k = 100\n",
    "    rf_estimators = 300\n",
    "    n_permutations = 200\n",
    "    perm_test_holdout_fraction = 0.2\n",
    "    n_jobs = -1\n",
    "    random_state = 42\n",
    "\n",
    "args = Args()\n",
    "\n",
    "os.makedirs(args.outdir, exist_ok=True)\n",
    "\n",
    "df = read_metadata(args.meta)\n",
    "print(\"[INFO] Metadata rows:\", len(df))\n",
    "radii = pd.to_numeric(df['RADIUS'], errors='coerce').dropna()\n",
    "median_radius = int(radii.median()) if radii.size>0 else 48\n",
    "print(\"[INFO] median_radius:\", median_radius)\n",
    "\n",
    "images = sorted([f for f in os.listdir(args.images) if f.lower().endswith('.pgm')])\n",
    "print(\"[INFO] Found images:\", len(images))\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "groups = []\n",
    "sample_rois = []\n",
    "\n",
    "for fname in images:\n",
    "    ref = os.path.splitext(fname)[0]\n",
    "    row = df[df['REFNUM'] == ref]\n",
    "    if row.empty:\n",
    "        # debug info: no metadata row\n",
    "        # print(f\"[DEBUG] No metadata for {ref}\")\n",
    "        continue\n",
    "    row = row.iloc[0]\n",
    "    img_path = os.path.join(args.images, fname)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(f\"[WARN] cannot read {img_path}, skipping\")\n",
    "        continue\n",
    "    img_eq = preprocess_img(img)\n",
    "    if img_eq is None:\n",
    "        print(f\"[WARN] preprocessed image too small: {img_path}\")\n",
    "        continue\n",
    "    roi = roi_from_row(img_eq, row, image_size=args.image_size, median_radius=median_radius, min_side=args.min_side)\n",
    "    roi_resized = cv2.resize(roi, (args.target_size, args.target_size), interpolation=cv2.INTER_AREA)\n",
    "    # compute features: GLCM first, then LBP, then intensity\n",
    "    glcm_feats = compute_glcm_features(roi_resized, distances=tuple(args.glcm_distances), angles=(0, math.pi/4, math.pi/2, 3*math.pi/4), levels=args.glcm_levels)\n",
    "    lbp_feats = compute_lbp_hist(roi_resized, P=args.P, radii=tuple(args.lbp_radii), n_bins=args.lbp_bins)\n",
    "    int_feats = intensity_stats(roi_resized)\n",
    "    feat_vec = np.concatenate([glcm_feats, lbp_feats, int_feats]).astype(float)\n",
    "    features.append(feat_vec)\n",
    "    labels.append(int(row['CANCER']))\n",
    "    groups.append(row['patient_id'])\n",
    "    if len(sample_rois) < 6:\n",
    "        sample_rois.append((ref, int(row['CANCER']), row['patient_id'], roi_resized))\n",
    "\n",
    "X = np.vstack(features) if len(features)>0 else np.zeros((0, args.lbp_bins*len(args.lbp_radii) + 5))\n",
    "y = np.array(labels, dtype=int)\n",
    "groups_arr = np.array(groups)\n",
    "print(\"[INFO] Extracted features shape:\", X.shape)\n",
    "print(\"Label distribution:\", Counter(y))\n",
    "\n",
    "with open(os.path.join(args.outdir, \"features.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"X\": X, \"y\": y, \"groups\": groups_arr, \"cfg\": vars(args)}, f)\n",
    "print(\"[INFO] Saved features.pkl\")\n",
    "\n",
    "# save sample rois preview\n",
    "plot_sample_rois(sample_rois, os.path.join(args.outdir, \"sample_rois.png\"))\n",
    "print(\"[INFO] Saved sample_rois.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66346653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: Training, nested group-aware CV and final model fit\n",
    "# Assumes features.pkl produced by previous cell\n",
    "with open(os.path.join(args.outdir, \"features.pkl\"), \"rb\") as f:\n",
    "    dd = pickle.load(f)\n",
    "X = dd['X']; y = dd['y']; groups_arr = dd['groups']\n",
    "\n",
    "n_groups = len(set([g for g in groups_arr if g is not None]))\n",
    "n_splits = min(5, max(2, n_groups))\n",
    "print(\"[INFO] Running GroupKFold with n_splits =\", n_splits)\n",
    "\n",
    "outer_cv = GroupKFold(n_splits=n_splits)\n",
    "inner_cv = GroupKFold(n_splits=max(2, n_splits-1))\n",
    "\n",
    "selector = SelectKBest(mutual_info_classif, k=min(args.select_k, X.shape[1]))\n",
    "scaler = StandardScaler()\n",
    "rf_clf = RandomForestClassifier(n_estimators=args.rf_estimators, class_weight='balanced', random_state=args.random_state)\n",
    "svm_clf = SVC(probability=True, class_weight='balanced', random_state=args.random_state)\n",
    "\n",
    "pipelines = {\n",
    "    'rf': Pipeline([('scaler', scaler), ('select', selector), ('clf', rf_clf)]),\n",
    "    'svm': Pipeline([('scaler', scaler), ('select', selector), ('clf', svm_clf)])\n",
    "}\n",
    "\n",
    "rf_grid = {'clf__n_estimators': [args.rf_estimators]}\n",
    "svm_grid = {'clf__C': [1.0], 'clf__gamma': ['scale']}\n",
    "\n",
    "results = {k: [] for k in pipelines.keys()}\n",
    "best_models = {}\n",
    "\n",
    "fold = 0\n",
    "for train_idx, test_idx in outer_cv.split(X, y, groups_arr):\n",
    "    fold += 1\n",
    "    Xtr, Xte = X[train_idx], X[test_idx]\n",
    "    ytr, yte = y[train_idx], y[test_idx]\n",
    "    gtr, gte = groups_arr[train_idx], groups_arr[test_idx]\n",
    "    print(f\"[INFO] Outer fold {fold}: train={len(train_idx)} test={len(test_idx)}\")\n",
    "\n",
    "    for name, pipe in pipelines.items():\n",
    "        param_grid = rf_grid if name=='rf' else svm_grid\n",
    "        gs = GridSearchCV(pipe, param_grid, cv=inner_cv, scoring='roc_auc', n_jobs=args.n_jobs, refit=True)\n",
    "        gs.fit(Xtr, ytr, groups=gtr)\n",
    "        best = gs.best_estimator_\n",
    "        ypred = best.predict(Xte)\n",
    "        yprob = best.predict_proba(Xte)[:,1] if hasattr(best, \"predict_proba\") else best.decision_function(Xte)\n",
    "        acc = accuracy_score(yte, ypred)\n",
    "        rec = recall_score(yte, ypred, zero_division=0)\n",
    "        auc = roc_auc_score(yte, yprob) if len(np.unique(yte))>1 else float('nan')\n",
    "        cm = confusion_matrix(yte, ypred)\n",
    "        results[name].append({'fold': fold, 'acc': acc, 'recall': rec, 'auc': auc, 'cm': cm, 'best_params': gs.best_params_})\n",
    "        best_models[name] = gs.best_estimator_\n",
    "        print(f\"[FOLD {fold}][{name}] acc={acc:.3f} rec={rec:.3f} auc={auc:.3f}\")\n",
    "\n",
    "# summarize and save report\n",
    "summary = {'n_samples': int(X.shape[0]), 'n_features': int(X.shape[1]), 'label_counts': dict(pd.Series(y).value_counts()), 'unique_groups': int(len(set(groups_arr)))}\n",
    "for name in results:\n",
    "    arr_acc = np.array([r['acc'] for r in results[name]])\n",
    "    arr_rec = np.array([r['recall'] for r in results[name]])\n",
    "    arr_auc = np.array([r['auc'] for r in results[name] if not np.isnan(r['auc'])])\n",
    "    summary[f'cv_{name}'] = {'mean_acc': float(np.nanmean(arr_acc)), 'std_acc': float(np.nanstd(arr_acc)), 'mean_rec': float(np.nanmean(arr_rec)), 'std_rec': float(np.nanstd(arr_rec)), 'mean_auc': float(np.nanmean(arr_auc)) if arr_auc.size>0 else None}\n",
    "\n",
    "# refit final models on full data and save\n",
    "for name, model in best_models.items():\n",
    "    try:\n",
    "        model.fit(X, y)\n",
    "        with open(os.path.join(args.outdir, f\"final_model_{name}.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "    except Exception as e:\n",
    "        print(\"Could not refit/save\", name, e)\n",
    "\n",
    "with open(os.path.join(args.outdir, \"report.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2, default=convert)\n",
    "\n",
    "print(\"[INFO] saved report.json and final models\")\n",
    "print(json.dumps(summary, indent=2, default=convert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: Permutation testing (group-wise). This uses a held-out group split.\n",
    "with open(os.path.join(args.outdir, \"features.pkl\"), \"rb\") as f:\n",
    "    dd = pickle.load(f)\n",
    "X = dd['X']; y = dd['y']; groups_arr = dd['groups']\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=args.perm_test_holdout_fraction, random_state=args.random_state)\n",
    "train_idx_perm, test_idx_perm = next(gss.split(X, y, groups_arr))\n",
    "Xtr_perm, Xte_perm = X[train_idx_perm], X[test_idx_perm]\n",
    "ytr_perm, yte_perm = y[train_idx_perm], y[test_idx_perm]\n",
    "gtr_perm, gte_perm = groups_arr[train_idx_perm], groups_arr[test_idx_perm]\n",
    "\n",
    "# Load final RF (if saved) or train fresh\n",
    "rf_path = os.path.join(args.outdir, \"final_model_rf.pkl\")\n",
    "if os.path.exists(rf_path):\n",
    "    with open(rf_path, \"rb\") as f:\n",
    "        rf_model = pickle.load(f)\n",
    "else:\n",
    "    rf_model = RandomForestClassifier(n_estimators=args.rf_estimators, class_weight='balanced', random_state=args.random_state)\n",
    "    rf_model.fit(Xtr_perm, ytr_perm)\n",
    "\n",
    "if hasattr(rf_model, \"predict_proba\"):\n",
    "    yprob = rf_model.predict_proba(Xte_perm)[:,1]\n",
    "else:\n",
    "    yprob = rf_model.decision_function(Xte_perm)\n",
    "\n",
    "real_auc = roc_auc_score(yte_perm, yprob) if len(np.unique(yte_perm))>1 else float('nan')\n",
    "perm_scores = []\n",
    "n_perm = args.n_permutations\n",
    "for i in range(n_perm):\n",
    "    # permute labels at the group level\n",
    "    perm_groups = np.unique(gtr_perm)\n",
    "    perm_map = np.random.permutation(perm_groups)\n",
    "    group_to_label = {g: int(pd.Series(ytr_perm[gtr_perm==g]).mode().iloc[0]) for g in perm_groups}\n",
    "    # create a permuted label vector for training by shuffling group labels\n",
    "    permuted_group_labels = np.random.permutation([group_to_label[g] for g in perm_groups])\n",
    "    ytr_p = ytr_perm.copy()\n",
    "    for g, lab in zip(perm_groups, permuted_group_labels):\n",
    "        ytr_p[gtr_perm==g] = lab\n",
    "    # fit clone\n",
    "    rf_clone = RandomForestClassifier(n_estimators=args.rf_estimators, class_weight='balanced', random_state=args.random_state)\n",
    "    rf_clone.fit(Xtr_perm, ytr_p)\n",
    "    try:\n",
    "        prob_p = rf_clone.predict_proba(Xte_perm)[:,1]\n",
    "        auc_p = roc_auc_score(yte_perm, prob_p) if len(np.unique(yte_perm))>1 else float('nan')\n",
    "    except:\n",
    "        auc_p = float('nan')\n",
    "    perm_scores.append(auc_p)\n",
    "\n",
    "pvalue = (np.sum(np.array(perm_scores) >= real_auc) + 1) / (len(perm_scores) + 1)\n",
    "print(\"[INFO] Permutation real_auc:\", real_auc, \"pvalue:\", pvalue)\n",
    "# Save sample of permutation scores\n",
    "with open(os.path.join(args.outdir, \"perm_scores_sample.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(perm_scores[:min(1000,len(perm_scores))], f)\n",
    "print(\"[INFO] saved perm_scores_sample.pkl\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
